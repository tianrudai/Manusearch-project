import queue, copy, re, uuid, json, time, logging, traceback
from ..schema import AgentMessage 
from ..utils.utils import *
from .planner import Planner
from .recorder import Recorder
from .searcher import Searcher
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%m-%d %H:%M:%S'
)
logging.getLogger("watchdog").setLevel(logging.INFO)


# Core search agent that coordinates multiple stages of search, reading, and reasoning
class SearchAgent:
    """
    A multi-agent framework for complex search tasks, making up the workflow between Planner, Searcher, Reader and Recorder.

    Args:
        planner (`Planner`): The planner agent responsible for decomposing the user query into sub-questions.
        searcher (`Searcher`): The searcher agent responsible for generating search queries and retrieving web results.
        reader (`Reader`): The reader agent responsible for parsing and summarizing search results.
        recorder (`Recorder`): The recorder agent responsible for maintaining the state of the search process.
        max_turn (`int`): The maximum number of iterations for the search process.

    Methods:
        iterative(`query`): Executes the search process iteratively, refining the search results over multiple turns.
    """
    def __init__(
        self,
        planner: Planner,
        searcher: Searcher,
        recorder: Recorder,
        llm,
        iterative_prompt,
        max_turn: int = 10
    ):
        """
        Initializes the SearchAgent with required components for each stage of the search process.
        
        Args:
            planner (`Planner`): Responsible for generating the overall plan.
            searcher (`Searcher`): Searches for relevant content based on subqueries.
            reader (`Reader`): Parses and summarizes the search results.
            recorder (`Recorder`): Records intermediate results and memories.
            max_turn (`int`): Maximum number of iterations (turns) for the process.
        """
        self.planner = planner
        self.searcher = searcher
        self.recorder = recorder
        self.max_turn = max_turn
        self.llm = llm
        self.iterative_prompt = iterative_prompt

    def forward(self, query, mode='iterative'):
    
        start_time = time.time()
        self.recorder.container['content'].add_root_node(node_content=query)
        try:
                self.planner.agent.system_prompt = self.iterative_prompt
                for response in self.iterative(query):
                    yield response
        except:
            formatted_messages = [
                {"role":"user", "content":query,"enable_thinking":False}            
            ]
            response = self.llm.chat(formatted_messages)
            yield {
                "final_resp": remove_think_tags(response.content),
                "stauts": "reasoning"
            }
            # raise
        cal_timediff(start_time)
        

    def iterative(self, query):
        """
        Executes the search process iteratively, refining the search results over multiple turns.

        Args:
            query (`str`): The user query to be processed.

        Returns:
            response(`AgentMessage`): The final response generated by the Reasoner.
        """
        planner_message = queue.Queue()
        planner_message.put(query)
        _graph_state = dict(node={}, adjacency_list={}, ref2url={})
        references_url = {}

        try:

            for turn in range(self.max_turn):
                # Plan the one search step.   User search query --> Overall search steps
                logging.info('planner planning....')
                with timeit("Planner"):

                    message = planner_message.get()
                    for response in self.planner.plan(
                        message=message,
                        recorder=self.recorder
                    ):
                        current_plan = parse_resp_to_json(response.content)

                        if isinstance(current_plan, dict) and 'actions' in current_plan:
                            if current_plan['actions'] == 'final_response':
                                yield {
                                    'final_resp': current_plan,
                                    'status': 'reasoning',
                                    'ref2url': references_url # global index
                                }
                            else:
                                yield {
                                    'plan': current_plan,
                                    'status': 'planning'
                                }

                # Execute search and summarize results for each sub-query
                
                if not finish_condition(current_plan) and current_plan['actions'] == 'extract_problems':
                    current_subquery = current_plan['content']
                    step_message = [] 
                    if isinstance(current_subquery, list):
                        current_subquery = current_subquery[-1]
                    logging.info('begin searching....')

                    with timeit("Searcher"):
                        for tool_name, search_result, references_url in self.searcher.search(
                            question=current_subquery, 
                            recorder=self.recorder,
                        ):
                            if tool_name == 'webpages':
                                yield {
                                    'status':'webpages',
                                    'content':search_result
                                }
                            else:
                                yield {
                                    'status': 'searching',
                                    'substatus': tool_name,
                                    'tool_return': search_result,
                                    'ref2url': references_url # global index
                                }
                                
                        _graph_state.update(node=self.recorder.container['content'].nodes, adjacency_list=self.recorder.container['content'].adjacency_list)
                        _graph_state['ref2url'].update(references_url)
                        if isinstance(search_result, dict):
                            if 'answer' in search_result:
                                search_result = search_result['answer']
                        # Store the summarized results
                        step_message.append(
                            AgentMessage(
                                sender="searcher",
                                content=search_result if search_result else 'can not find realted information!',
                                formatted=copy.deepcopy(_graph_state)
                            )
                        )

                    planner_message.put(step_message)

                elif finish_condition(current_plan):
                    response.formatted = _graph_state
                    return response

                else:
                    step_message = f"Error: {current_plan['evaluation_previous_goal']}."
                    planner_message.put(step_message)

            # reason after max_turn
            if not finish_condition(current_plan):
                logging.info('begin reasoning after max_turn....')
                with timeit("Reasoning"):
                    message="Maximum number of rounds exceeded, please answer user questions immediately based on information already collected"
                    for response in self.planner.plan(message=message, recorder=self.recorder):
                        reason_message = parse_resp_to_json(response.content)
                        yield {
                            'final_resp': reason_message, 
                            'status': 'reasoning',
                            'ref2url': references_url # global index
                        }
                    response.formatted = _graph_state
                    return response
                
        except Exception:
            print('Stack trace:', traceback.format_exc())
            raise